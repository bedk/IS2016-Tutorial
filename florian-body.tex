\section{Research Topics, Challenges, and New Ideas}

% bedk - \par at end ensures proper line spacing
\begin{frame}
  \begin{center}
    {\color{Maroon}\Huge Research Directions and New Modeling Techniques\par}
  \end{center}
\end{frame}

%%%% ----------------------------
%%%%     CHALLENGES
%%%% ----------------------------

\begin{frame}
  \begin{center}
    {\color{Maroon}\Huge Challenges}
  \end{center}
\end{frame}

\begin{frame}{Are we there yet?}
  \begin{center}
    \includegraphics[height=65mm]{figures/ASR9}
  \end{center}
\end{frame}

\begin{frame}{Are we there yet (2)?}
  \begin{center}
    \includegraphics[height=65mm]{figures/am-mlp}
  \end{center}
\end{frame}

\begin{frame}{Challenges}{What could be wrong?}
  \begin{itemize}
  \item We are using a solid statistical framework
  \item We know how to extract ``features''
  \item We know how to learn models
  \item We know how to search for the best path
  \item So we must be doing the right thing!
  \end{itemize}
\end{frame}

\begin{frame}{Really?}{Let's think about this some more!}
  \begin{itemize}
    \item Given: an observation sequence (ADC, FFT, ...)\\
      $\boldsymbol{X} = \boldsymbol{x}_1, \boldsymbol{x}_2, ..., \boldsymbol{x}_T$
    \item Wanted: the corresponding word sequence\\
      $W = w_1, w_2, ..., w_m$
    \item Search for the most likely word sequence $W'$
    \item Fundamental Equation of ASR: \\
      $W' = \arg \max_W P(W|\boldsymbol{X}) = \arg \max p(\boldsymbol{X}|W) P(W) $ \hspace{1cm} (Bayes)
    \item $p(\boldsymbol{X}|W)$ is the ``acoustic model''
    \item $P(W)$ is the ``language model''
    \item And how are we evaluating this?
  \end{itemize}
\end{frame}

\begin{frame}{Word Error Rate (WER)}
  \begin{itemize}
  \item \#errors / \#words (in reference)
  \item (\#insertions + \#substitutions + \#deletions) / \#words
  \item Example:\vspace{2mm}\\
    \begin{tabular}{lccccc}
      Reference  &            & \texttt{SHOW} & \texttt{ME} & \texttt{THE} & \texttt{INTERFACE} \\
      Hypothesis & \texttt{I} & \texttt{SHOW} & \texttt{ME} &              & \texttt{FACE} \\
      Alignment  & \textsc{I} &               &             & \textsc{D}   & \textsc{S} \\
    \end{tabular}
  \item WER=$3/4=75\%$
  \item Aha! Does WER appear anywhere in the fundamental equation?
  \end{itemize}
\end{frame}

\begin{frame}{Fundamentals Revisited}
  \begin{itemize}
  \item If we are optimizing for $P(W|\boldsymbol{X})$,
    we are optimizing for the \textit{sentence} error rate: $\langle P(W|\boldsymbol{X}) \rangle$ with $W \in \{W\}$
  \item But we want $P(\langle w_i \rangle|\boldsymbol{X})$ for $w_i \in \{ w_1,w_2, ..., w_m \} \equiv \{W\}$ (for one sentence)
  \item Clearly, this is not the same thing!
  \item However, in practice, it is not complete uncorrelated either
    \begin{itemize}
    \item[$\rightarrow$] And there are ways to deal with this (confusion networks, discriminative training)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{The Speech Recognition Conundrum}
  \begin{itemize}
  \item We want to optimize word error rate (or whatever)\\
    \vspace{1cm}
  \item We optimize the acoustic model for (frame-)likelihood (or cross-entropy, or MPE, sMBR, ...)
  \item We train a language model for perplexity
  \item And then we search for the most likely path (a ``sentence'')
  \item Similarly, for spoken term detection, dialog systems, etc.
    \begin{itemize}
    \item We are optimizing different components for different criteria, and shoe-horn them together
    \end{itemize}
  \item And this works?
    \begin{itemize}
    \item By and large, yes!
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{The Speech Recognition Conundrum}{It's Ugly!}
  \begin{center}
    \includegraphics[height=65mm]{figures/medusa}
  \end{center}
\end{frame}

\begin{frame}{Do we really need Hidden Markov Models?}
  \begin{itemize}
  \item We know all the theory of why HMMs are good for modelling time series
  \item We have \#states, initial probability distribution, emission probabilities,
    transition probabilities, and the alphabet to define
  \item Many different ``types'' of parameters that we can play with, until we believe we have a good fit
  \item We typically neglect transition probabilities in practice
    \begin{itemize}
    \item We set all probabilities to 1 (violating sum-to-one condition)
    \end{itemize}
  \end{itemize}
  \begin{center}
    \includegraphics[clip=true,trim=8cm 5.65cm 17.5cm 9.5cm]{figures/am-gmm.pdf}
  \end{center}
\end{frame}

\begin{frame}{Observed Phone Durations}
  \begin{center}
    \includegraphics[height=50mm]{figures/durations}
  \end{center}
  \tiny From \cite{Pylkkonen04durationmodeling}
\end{frame}

\begin{frame}{Phone Durations with a single-state HMM}
  \begin{center}
    \includegraphics[height=50mm]{figures/durations3}
  \end{center}
  \begin{itemize}
  \item A single HMM state has an exponentially decreasing duration distribution.
%  \item A typical tri-state topology (\textsc{-b -m -e} states) at least has a ``maximum'' peak for
%    the expected duration of the whole phone
  \end{itemize}
\end{frame}

\begin{frame}{Duration Modelling with HMMs}
  \begin{itemize}
  \item A tri-state topology (\textsc{-b -m -e} states) at least has a ``maximum'' peak for
    the expected duration of the whole phone
  \item It is still quite arbitrary - we have short vowels, long vowels, plosives, word-final nasals, ...\\
    \hspace{1cm}
  \item \textbf{In short}: \textit{let's not make explicit model assumptions
    that aren't really justified, and that we cannot train the parameters for}
    \begin{itemize}
    \item (I hope I am not insulting anyone, but:) duration modeling has never shown
      worthwhile, consistent gains\\
      \hspace{1cm}\\
      \hspace{1cm}
    \end{itemize}
  \item \textbf{Maybe}, \textit{we should not be using HMMs after all}
    \begin{itemize}
    \item There are a few more problems like hyper-articulation that we do not know how to handle well
    \end{itemize}
  \end{itemize}
\end{frame}

%\begin{frame}{Ok}{What Do We Do?}
%  \begin{itemize}
%  \item There are a few more problems (e.g., hyper-articulation)
%    \begin{itemize}
%    \item Try talking ``extra clearly'' to a dialogue system
%    \item It won't get you anywhere ...
%    \end{itemize}
%  \item Speech recognition ``works''
%  \item Airplanes don't fly the same way birds fly
%  \item We may be fine (if we adapt)
%  \item Still, what could we do?
%  \end{itemize}
%\end{frame}

\begin{frame}{Ok, what to do?}{Let's take a step back}
  \begin{itemize}
  \item What are we really trying to achieve?
  \item We want to translate a sequence of observations into a sequence of words
  \item And ideally, we want to use a single optimality criterion for that
  \item In 2016? In San Francisco?
  \end{itemize}
\end{frame}

\begin{frame}{There must be a Deep Learning solution for this!}{\cite{s2s-nips}}
  \begin{center}
    \includegraphics[height=65mm]{figures/s2s}
  \end{center}
\end{frame}

\begin{frame}{Easy, isn't it?}{\cite{s2s-nips}}
  \begin{center}
    \includegraphics[height=35mm]{figures/s2s-fig}
  \end{center}
\hspace{1.7cm} $\rightarrow$ SPEECH $\mapsto$ TXET $\rightarrow$
\end{frame}

\begin{frame}{End-to-end models with attention}{What happens when we apply this to speech?}
  \begin{itemize}
  \item ``Sequences'' in speech are much longer than in text (even at 30Hz)
    \begin{itemize}
    \item The ``encoder's'' task is therefore much harder
    \end{itemize}
  \item However there is no re-ordering going on (unlike in Machine Translation)
  \item Maybe we could introduce a mechanism that gives ``hints'' to the decoder based on how much of the input has already been consumed
    \begin{itemize}
    \item Turns out, ``attention'' mechanisms are well suited to recover from decoding errors
    \item Imagine the attention mechanism ``guiding'' an RNN language model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{End-to-end models with attention}{\cite{BahdanauCSBB15}}
  \begin{center}
    \includegraphics[height=55mm]{figures/att}
  \end{center}
\end{frame}

\begin{frame}{And this works? What's the WER?}
  \begin{itemize}
  \item According to ``Listen, Attend and Spell'' (\cite{las}):
    \begin{itemize}
    \item Google says they're close on in-house task (10.3\% vs.\ 8.0\%)
    \item 1000s of hours of training data
    \item ``Insane'' amount of computation [personal communication]
    \end{itemize}
  \item Other work emerging, still showing higher word error rates
  \item But this is probably just the tip of the ice-berg
    \begin{itemize}
    \item If you can go from images to text, you can surely go from speech to text
      \item Optimization for other criteria and joint learning certainly possible
    \end{itemize}
  \item What have we gained?
    \begin{itemize}
    \item There are almost no explicit assumptions in our model
    \item Everything becomes a Deep Learning hyper-parameter
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{So, why did we not start here?}{Is this where it ends?}
  \begin{itemize}
  \item Problem with end-to-end learning? It is end-to-end!
  \item How to go from one language to another, from one domain to the next?
  \item Who knows. Topic for a NIPS 2020 Tutorial? Or maybe 2018?
    \begin{itemize}
    \item Condition the decoder on something?
    \item Find clever embeddings?
    \item ...?
    \end{itemize}
  \item Are there alternatives? Still (somewhat) end-to-end, neural?
  \end{itemize}
\end{frame}

%\begin{frame}{Does it end here?}
%  \begin{itemize}
%  \item No!!!
%  \item Alternatives? Still (somewhat) end-to-end, neural?
%  \item Connectionist Temporal Classification \& friends
%  \end{itemize}
%\end{frame}

\begin{frame}{Connectionist Temporal Classification}
  \begin{itemize}
  \item We want to stay sequence-to-sequence
    \begin{itemize}
    \item Do not bring back any of the ``traditional'' ballast
    \item In particular the CD-HMM complexity
    \end{itemize}
  \item Separate the acoustics and the language model
    \begin{itemize}
      \item That's just convenient and pragmatic (even though it's maybe no longer strictly end-to-end)
      \item So we want an acoustic model that assumes neighboring units to be independent
    \end{itemize}
  \item We don't need re-ordering and stuff
    \begin{itemize}
    \item We don't even really care about time information
    \item Speech is continuous and dynamic, maybe there should not be any lasting ``states''
    \end{itemize}
  \item Remember what we said about flat start?
    \begin{itemize}
    \item Maybe we can put all this in the model?
    \item And not deal with frame likelihoods etc.\ in too much detail
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Connectionist Temporal Classification (CTC)}
  \begin{itemize}
  \item Alex Graves (2006) described the ``CTC'' loss function
    \begin{itemize}
    \item Defined over a label sequence (length $M$)
    \item Sum over all possible frame alignments (length $T$) permitted for label sequence using Forward-Backward
    \end{itemize}
  \item CTC introduces a new symbol: blank (-)
    \begin{itemize}
    \item ``No output''
    \item Do not confuse with ``silence''
    \end{itemize}
  \item Most of the time, the network will output (-)
    \begin{itemize}
    \item ``Sparse'' model, not dense in time
    \item Class im-balance not a problem in a connectionist architecture
    \item As long as the target symbols appear from time to time
    \end{itemize}
  \item Plays well with RNN or LSTM neural network models
  \end{itemize}
\end{frame}

\begin{frame}{Connectionist Temporal Classification}
  \begin{itemize}
  \item How does this work?
  \item The probability of recognizing a ``label'' is the sum of all permitted paths
    (over ``frames'') that correspond to that ``label''
  \item Example: label $\boldsymbol{l}$ is \texttt{A}
    \begin{itemize}
    \item Permitted paths $\pi$ for \texttt{A} are \textsc{a}, \textsc{-a}, \textsc{{-}-aaa}, ...
    \item But not \textsc{a-a}
    \end{itemize}
  \item Example: label sequence $\boldsymbol{l}$ is \texttt{AB}
    \begin{itemize}
    \item Permitted paths $\pi$ for \texttt{AB} are e.g.\,\textsc{ab}, \textsc{{-}-{-}-aaab-{-}}, \textsc{-a-bb-{-}-}, ...
    \item \texttt{AA} has to be \textsc{a-a}, but \texttt{AB} can be \textsc{ab} or \textsc{a-b}
    \end{itemize}
  \item A grammar that collapses repetitions and deletes blanks
    \begin{itemize}
    \item Non bijective map $\mathcal{B}$ between paths $\pi$ and label sequences $\boldsymbol{l}$
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{CTC Loss Function}
  \begin{align}
    p(\boldsymbol{l}|\boldsymbol{X})   & = \sum_{\pi \in \mathcal{B}^{-1}(\boldsymbol{l})} p(\pi|\boldsymbol{X})\\
    p(\pi|\boldsymbol{X}) & = \prod_{t=1}^T y^t_{\pi_t}, \forall \pi \in L'^T
  \end{align}
  \begin{itemize}
  \item $y^t_{\pi_t}$ is the output of the network at time $t$ (according to the path)
  \item The likelihood of a labeling $p(\boldsymbol{l}|\boldsymbol{X})$ can be computed efficiently using Fwd-Bwd over all permitted paths, using prefixes/ postfixes
  \item Also introduce an augmented label sequence $\boldsymbol{l'}$ which includes optional blanks
  \end{itemize}
\end{frame}

\begin{frame}{CTC Objective Function}
  \begin{align}
    O^{ML}(S,\mathcal{N}_W)&=-\sum_{(x,l) \in S} ln(p(l|x))
  \end{align}
  \begin{itemize}
  \item Loss function is neg-log prob of correctly labeling all observation/ labeling pairs $(x,l) \in S$
  \item $y = \mathcal{N}_w (x)$ is the function that the network learns
  \item Consider each sequence pair independently
  \item Interestingly, the error can be backpropagated through this easily
  \end{itemize}
\end{frame}
% TODO: check this

\begin{frame}{What does this look like?}
  \begin{center}
    \includegraphics[height=35mm]{figures/ctc}
  \end{center}
  \tiny From \cite{graves2006connectionist}
\end{frame}

\begin{frame}{Efficient Decoding with CTC}
  \begin{itemize}
  \item Originally, it was not clear how these things could be decoded efficiently
  \item Turns out, a wFST (weighted Finite State Transducer) framework works just fine \cite{eesen,sak2015fast}
  \item Need to carefully design the $T$ transducers for the tokens, which however replace the $H$ (HMM) and $C$ (context) transducers in conventional HMM-based systems
  \item Get class posteriors directly from label counts
  \item Advantage(s) at comparable accuracy
    \begin{itemize}
    \item CTC system is much smaller and usually faster during decoding
    \item Can use 30ms frame rate and works well with graphemic units
    \item N.B.: some of these findings might not be due to CTC, but to LSTMs, and may be back-ported
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Research Topics}
  \begin{itemize}
  \item Can we make ASR simpler?
    \begin{itemize}
    \item Yes!!! Very much so.
    \end{itemize}
  \item Can we build multi-lingual systems, use multi-task training?
    \begin{itemize}
    \item Yes, you can. In your tutorial VM!
    \end{itemize}
  \item Can we use this to improve phone recognition
    \begin{itemize}
    \item Yes!
    \end{itemize}
  \item What does this mean for spoken term detection?
    \begin{itemize}
    \item The lack of reliable timings is a problem
    \end{itemize}
  \item Do end-to-end approaches work in low resource scenarios?
    \begin{itemize}
    \item 10s of hours of training data, rather than 1000s? But seems ok!
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{New Ideas}
  \begin{itemize}
  \item Combine CTC and attention-based models
    \begin{itemize}
    \item Use CTC as a front-end to attention-based models
    \item A la language independent feature extractors?
    \item Makes it easier for an encoder/ decoder model to learn
    \item Across languages?
    \end{itemize}
  \item Integrate non-auditory information
    \begin{itemize}
    \item Multimedia data is very rich, e.g.\,video
    \item Extract objects, scenes, speaker info from the visual part
    \item Use it to bootstrap, adapt, condition, or otherwise improve acoustic and/ or language models
    \end{itemize}
  \item Go beyond the speech-to-text paradigm
    \begin{itemize}
    \item Maybe a verbatim transcription is not always what we want
    \item Combine different information sources intelligently to give us audio-to-meaning in a new domain or language
    \end{itemize}
  \end{itemize}
\end{frame}


\section{Hands-On with Virtual Machines}

\begin{frame}
  \begin{center}
    {\color{Maroon}\Huge Hands-On Experience with Virtual Machines\par}
  \end{center}
\end{frame}

\begin{frame}{Practicalities}
  \begin{itemize}
  \item We want to give you hands-on experience with building ASR systems
  \item Immediately start training a system on IARPA-babel201b-v0.2b (Haitian Creole)
  \item You can then experiment with other Babel languages, or port the system to other domains
  \item To facilitate experimentation, we will use a Virtual Machine, and/ or an AMI
  \item {\color{Maroon}Read on to see how you can prepare (or listen ...)}
  \end{itemize}
\end{frame}

\begin{frame}{Virtual Machines (VMs) and other Tools}
  \begin{itemize}
  \item Think of a VM as a ``virtual'' computer, in our case running Linux
  \item VMs allow sharing reproducible experiments easily
  \item \url{https://github.com/srvk}, \url{http://speechkitchen.org} as repositories
  \item \url{https://www.vagrantup.com/} to build VMs and AMIs (Amazon Machine Images)
  \item Run VMs with Virtualbox (\url{https://www.virtualbox.org/}) or on AWS \url{https://aws.amazon.com/}
  \item Your ``virtual computer'' is an ``image'' when it is turned off, it becomes an ``instance'' when you turn it on
  \end{itemize}
\end{frame}

\begin{frame}{Hands-on Experience in this Tutorial}{The Speech Recognition Virtual Kitchen}
  \begin{itemize}
  \item Please visit \url{https://github.com/srvk/aws-sandbox}
  \item There you can find a \textsc{Vagrantfile}, to build a VM (e.g. an OVA)
  \item But we recommend running things on AWS, with an \textsc{AMI} that we provide there
    \begin{itemize}
    \item So, you may want to sign up for an account first (\url{https://aws.amazon.com/getting-started/})
    \item Familiarize yourself with how to start a Linux VM on ``EC2'' using a pre-configured Amazon Machine Image (AMI)
    \end{itemize}
  \item Training a DNN-based recognizer on a GPU will cost some money, \textasciitilde \$25 (you may have credits)
  \item Once you reproduced the basics, you can continue on AWS, or you can migrate to your own infrastructure
  \end{itemize}
\end{frame}

\begin{frame}{What's in the Tutorial VM?}{A ready-to-go, open source platform for ASR experiments}
  \begin{columns}[c]
    \column{52mm}
    \begin{itemize}
    \item Haitian Creole data from the Babel program
    \item We will use the ``Eesen'' toolkit (\url{https://github.com/srvk/eesen}) for CTC-based end-to-end speech recognition
    \item It borrows from Kaldi (\url{http://kaldi-asr.org/}), but is a bit smaller and easier to handle
    \item More tools like KenLM (\url{https://kheafield.com/code/kenlm/}) etc.
    \end{itemize}
    \column{58mm}
    \includegraphics[width=58mm]{figures/detox}    
  \end{columns}
\end{frame}

\begin{frame}{Exercise}{Go to \url{https://github.com/srvk/aws-sandbox}}
  \begin{itemize}
  \item Sign the data use agreement at the end of the tutorial
  \item Send your name and AWS Account ID (a 12-digit number) to \href{mailto:fmetze@cs.cmu.edu}{fmetze@cs.cmu.edu}
    \begin{itemize}
    \item We will give you permissions to the AWS AMI referenced on Github
    \item It contains (free and exclusive to participants of this tutorial) Babel data (Haitian Creole) 
    \item Thanks to IARPA and LDC
    \end{itemize}
  \item Start up the provided AMI in the US-East zone on a GPU node (g2.2xlarge)
  \item Or build the VM/ AMI yourself
    \begin{itemize}
    \item Check out the \texttt{master} branch (for now, we may add tags)
    \item Bring your own data
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Exercise}{Go to \url{https://github.com/srvk/aws-sandbox}}
  \begin{itemize}
  \item Fire up the AMI (VM)
  \item Log in as \texttt{ec2-user}, change to \texttt{\textasciitilde/babel-exps/egs/asr/s5c/201-haitian-flp}
  \item Run the following commands:
    \begin{enumerate}
    \item \texttt{nohup ./run-1-main.sh    >\& log/run.log \&}
      \begin{itemize}
      \item Wait very few hours (data preparation)
      \end{itemize}
    \item \texttt{nohup ./train-7-gpu.sh   >\& log/train.log \&}
      \begin{itemize}
      \item Wait a day (model training)
      \end{itemize}
    \item \texttt{nohup ./test-7-v1\_x3.sh >\& log/test.log \&}
      \begin{itemize}
      \item Have coffee (model testing)
      \end{itemize}
    \end{enumerate}
  \item ... making sure each terminates before running the next one
  \end{itemize}
\end{frame}

\begin{frame}{Ideas for own Experiments}
  \begin{itemize}
  \item Train systems in other languages (get them from LDC for \$25 each)
    \begin{itemize}
    \item Scripts are all set up for this
    \end{itemize}
  \item Train multi-lingual systems (scripts provided)
  \item Improve, adapt, ... any of the systems
  \item ...
  \item Come to the SRVK Special Session
    \begin{itemize}
    \item SHARING RESEARCH AND EDUCATION RESOURCES FOR UNDERSTANDING SPEECH PROCESSING
    \item Saturday, Sep 10; 13:30 - 15:30; Grand Ballroom BC
    \item We'll discuss results and experiences in the coffee break after the session!
    \end{itemize}
  \item {\color{Maroon} Connect with us on Github}
  \end{itemize}
\end{frame}

%\begin{frame}
%  \frametitle{References}    
%  \cite{quesst:icassp2015,metze:is2015,yajie-lstm:is2015,yajie-robust:is2015,yashesh:is2015,yajie:taslp2015,eesen,trecvid:2015,eesen-icassp,wang2016icassp,w4a:2016,icmr2016,miao:is2016,vms:is2016,shared:is2016,yash:is2016,e2echapter,dnnbook}
%\cite{dnnbook}
%\end{frame}
