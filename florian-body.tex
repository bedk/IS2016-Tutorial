\section{Research Topics, Challenges, and New Ideas}

% bedk - \par at end ensures proper line spacing
\begin{frame}
  \begin{center}
    {\color{Maroon}\Huge Research Directions and New Modeling Techniques\par}
  \end{center}
\end{frame}

%%%% ----------------------------
%%%%     CHALLENGES
%%%% ----------------------------

\begin{frame}
  \begin{center}
    {\color{Maroon}\Huge Challenges}
  \end{center}
\end{frame}

\begin{frame}{Are we there yet?}
  \begin{center}
    \includegraphics[height=65mm]{figures/ASR9}
  \end{center}
\end{frame}

\begin{frame}{Is that it?}
  \begin{center}
    \includegraphics[height=65mm]{figures/am-mlp}
  \end{center}
\end{frame}

\begin{frame}{Challenges}
  \begin{itemize}
  \item What could be wrong?
  \item We are using a solid statistical framework
  \item We know how to extract ``features''
  \item We know how to learn models
  \item We know how to search for the best path
  \item So we must be doing the right thing!
  \end{itemize}
\end{frame}

\begin{frame}{Really? Let's think about this some more!}
  \begin{itemize}
    \item Given: an observation (ADC, FFT, ...)\\
      $\boldsymbol{X} = \boldsymbol{x}_1, \boldsymbol{x}_2, ..., \boldsymbol{x}_T$
    \item Wanted: the corresponding word sequence\\
      $W = w_1, w_2, ..., w_m$
    \item Search for the most likely word sequence $W'$
    \item Fundamental Equation of ASR: \\
      $W' = \arg \max_W P(W|\boldsymbol{X}) = \arg \max p(\boldsymbol{X}|W) P(W) $ \hspace{1cm} (Bayes)
    \item $p(\boldsymbol{X}|W)$ is the ``acoustic model''
    \item $P(W)$ is the ``language model''
    \item And how are we evaluating this?
  \end{itemize}
\end{frame}

\begin{frame}{Word Error Rate}
  \begin{itemize}
    \item \#errors / \#words (in reference)
    \item (\#insertions + \#substitutions + \#deletions) / \#words
    \item Example:\\
      \begin{tabular}{lccccc}
        Reference  &            & \texttt{SHOW} & \texttt{ME} & \texttt{THE} & \texttt{INTERFACE} \\
        Hypothesis & \texttt{I} & \texttt{SHOW} & \texttt{ME} &              & \texttt{FACE} \\
        Alignment  & \textsc{I} &               &             & \textsc{D}   & \textsc{S} \\
      \end{tabular}
    \item WER=$3/4=75\%$
    \item So? Did anyone see this in the fundamental equation? $P(W|\boldsymbol{X})$, anyone?
  \end{itemize}
\end{frame}

\begin{frame}{Fundamentals revisited}
  \begin{itemize}
  \item If we are optimizing for $P(W|\boldsymbol{X})$,
    we are optimizing for the \textit{sentence} error rate, $\langle P(W|\boldsymbol{X}) \rangle$
  \item But we want $P(\langle w_i \rangle|\boldsymbol{X})$ for $W=w_1,w_2, ..., w_m$ (for one sentence)
  \item Clearly, this is not the same thing!
  \item However, in practice, it is not complete uncorrelated either
    \begin{itemize}
    \item[$\rightarrow$] And there are ways to deal with this (confusion networks, discriminative training)
    \end{itemize}
  \item Similarly, for spoken term detection, dialog systems, etc.
  \end{itemize}
\end{frame}

\begin{frame}{The speech recogition conundrum}
  \begin{itemize}
  \item We want to optimize word error rate (or whatever)\\
    \vspace{1cm}
  \item We optimize the acoustic model for (frame-)likelihood (or cross-entropy, or MPE, sMBR, ...)
  \item We train a language model for perplexity
  \item And then we search for the most likely path (a ``sentence'')
  \item And this works?
  \end{itemize}
\end{frame}

\begin{frame}{The speech recognition conundrum - It's ugly!}
  \begin{center}
    \includegraphics[height=65mm]{figures/medusa}
  \end{center}
\end{frame}

\begin{frame}{Do we really need Hidden Markov Models?}
  \begin{itemize}
  \item We know all the theory of why HMMs are good for modelling time series
  \item We have \#states, initial probability distribution, emission probabilities,
    transition probabilities, and the alphabet to define
  \item Many different ``types'' of parameters that we can play with, until we believe we have a good fit
  \item In practice, we already neglect transition probabilities
  \item The probability of staying in a state decays exponentially over time
  \end{itemize}
\end{frame}

\begin{frame}{Observed Phone Durations}
  \begin{center}
    \includegraphics[height=50mm]{figures/durations}
  \end{center}
  \tiny From \cite{Pylkkonen04durationmodeling}
\end{frame}

\begin{frame}{Phone Durations with a single-state HMM}
  \begin{center}
    \includegraphics[height=50mm]{figures/durations3}
  \end{center}
  \begin{itemize}
  \item A single HMM state has an exponentially decreasing duration distribution.
  \item A typical tri-state topology (\textsc{-b -m -e} states) at least has a ``maximum'' peak for
    the expected duration of the whole phone
  \end{itemize}
\end{frame}

\begin{frame}{Duration Modelling with HMMs}
  \begin{itemize}
  \item A tri-state topology (-b -m -e states) at least has a ``maximum'' peak for
    the expected duration of the whole phone
  \item It is still quite arbitrary - we have plosives, short vowels, long vowels, ...\\
    \hspace{1cm}
  \item \textbf{In short}: \textit{let's not make explicit model assumptions
    that aren't really justified, and that we cannot train the parameters for}
    \begin{itemize}
    \item (I hope I am not insulting anyone, but:) duration modeling has never shown
      worthwhile, consistent gains\\
      \hspace{1cm}\\
      \hspace{1cm}
    \end{itemize}
    \item \textbf{Maybe}, \textit{we should not be using HMMs after all}
  \end{itemize}
\end{frame}

\begin{frame}{Ok, so what do we do?}
  \begin{itemize}
  \item There are a few more problems (e.g., hyper-articulation)
  \item Speech recognition ``works''
  \item Airplanes don't fly the same way birds fly
  \item We may be fine
  \item Still, what could we do?
  \end{itemize}
\end{frame}

\begin{frame}{Let's take a step back}
  \begin{itemize}
  \item What are we really trying to achieve?
  \item We want to translate a sequence of observations to a sequence of words
  \item And ideally, we want to use a single optimality criterion for that
  \item In 2016? In San Francisco?
  \end{itemize}
\end{frame}

\begin{frame}{There must be a Deep Learning solution for this!}
  \begin{center}
    \includegraphics[height=65mm]{figures/s2s}
  \end{center}
\end{frame}

\begin{frame}{Easy, isn't it?}
  \begin{center}
    \includegraphics[height=35mm]{figures/s2s-fig}
  \end{center}
\hspace{1.7cm} $\rightarrow$ SPEECH $\mapsto$ TXET $\rightarrow$
\end{frame}

\begin{frame}{End-to-end models with attention}
  \begin{itemize}
  \item ``Sequences'' in speech are much longer than in text (even at 30Hz)
  \item The ``encoder'''s task is therefore much harder
  \item However there is no re-ordering going on (unlike in Machine Translation)
  \item So, ``attention'' mechanisms are well suited to recover from decoding errors
    \begin{itemize}
    \item Imagine the attention mechanism ``guiding'' an RNN language model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{End-to-end models with attention}
  \begin{center}
    \includegraphics[height=55mm]{figures/att}
  \end{center}
  \tiny From \cite{BahdanauCSBB15}
\end{frame}

\begin{frame}{And this works? What's the WER?}
  \begin{itemize}
  \item According to ``Listen, Attend and Spell'' (\cite{las}):
    \begin{itemize}
    \item Google says they're close on their task (10.3\% vs.\ 8.0\%)
    \item 1000s of hours of training data
    \item ``Insane'' amount of computation [personal communication]
    \end{itemize}
  \item Other work emerging, still showing higher word error rates
  \item But this is probably just the tip of the ice-berg.
    \begin{itemize}
    \item If you can go from images to text, you can surely go from speech to text
      \item Optimization for other criteria and joint learning certainly possible
    \end{itemize}
  \item What have we gained?
    \begin{itemize}
    \item There are almost no explicit assumptions in our model
    \item Everything becomes a Deep Learning hyper-parameter
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{So, why did we not start here?}
  \begin{itemize}
  \item Problem with end-to-end learning? It is end-to-end!
  \item How to go from language to another, from one domain to the next?
  \item Who knows. Topic for a NIPS 2020 Tutorial (or maybe 2018)
    \begin{itemize}
    \item Condition the decoder on something?
    \item Find clever embeddings?
    \item ...?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Does it end here?}
  \begin{itemize}
  \item No!!!
  \item Alternatives? Still (somewhat) end-to-end, neural?
  \item Connectionist Temporal Classification \& friends
  \end{itemize}
\end{frame}

\begin{frame}{Connectionist Temporal Classification}
  \begin{itemize}
  \item We want to stay sequence-to-sequence
    \begin{itemize}
    \item Do not bring back any of the ``traditional'' ballast
    \item In particular the CD-HMM complexity
    \end{itemize}
  \item Separate the acoustics and the language model
    \begin{itemize}
      \item That's just convenient and pragmatic (even though it's maybe no longer strictly end-to-end)
      \item So we want an acoustic model that assumes neighboring units to be independent
    \end{itemize}
  \item We don't need re-ordering and stuff
    \begin{itemize}
    \item We don't even really care about time information
    \item Speech is continuous and dynamic, maybe there should not be any lasting ``states''
    \end{itemize}
  \item Remember what we said about flat start?
    \begin{itemize}
    \item Maybe we can put all this in the model?
    \item And not deal with frame likelihoods etc.\ in too much detail
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Connectionist Temporal Classification (CTC)}
  \begin{itemize}
  \item Alex Graves (2006) described the ``CTC'' loss function
    \begin{itemize}
    \item Defined over a label sequence (length $M$)
    \item Sum over all possible frame alignments (length $T$) permitted for label sequence using Forward-Backward
    \end{itemize}
  \item Plays well with RNN or LSTM neural network models
  \item CTC introduces a new symbol: blank (-)
    \begin{itemize}
    \item ``No output''
    \item Do not confuse with ``silence''
    \end{itemize}
  \item Most of the time, the network will output (-)
    \begin{itemize}
    \item ``Sparse'' model, not dense in time
    \item Class im-balance not a problem in a connectionist architecture
    \item As long as the target symbols appear from time to time
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Connectionist Temporal Classification}
  \begin{itemize}
  \item How does this work?
  \item The probability of recognising a ``label'' is the sum of all permitted paths
    (over ``frames'') that correspond to that ``label''
  \item Example: label (sequence) $\boldsymbol{l}$ is \textsc{A}
    \begin{itemize}
    \item Permitted paths $\pi$ for \textsc{A} are \textsc{a}, \textsc{-a}, \textsc{{-}-aaa}, ...
    \item But not \textsc{a-a}
    \end{itemize}
  \item Example: label sequence $\boldsymbol{l}$ is \textsc{AB}
    \begin{itemize}
    \item Permitted paths $\pi$ for \textsc{AB} are e.g.\,\textsc{ab}, \textsc{{-}-{-}-aaab-{-}}, \textsc{-a-bb-{-}-}, ...
      \item \textsc{AA} has to be \textsc{a-a}, but \textsc{AB} can be \textsc{ab} or \textsc{a-b}
    \end{itemize}
  \item A grammar that collapses repetitions and deletes blanks
    \begin{itemize}
    \item Non bijective map $\mathcal{B}$ between paths $\pi$ and label sequences $\boldsymbol{l}$
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{CTC Loss Function}
  \begin{align}
    p(\boldsymbol{l}|\boldsymbol{X})   & = \sum_{\pi \in \mathcal{B}^{-1}(\boldsymbol{l})} p(\pi|\boldsymbol{X})\\
    p(\pi|\boldsymbol{X}) & = \prod_{t=1}^T y^t_{\pi_t}, \forall \pi \in L'^T
  \end{align}
  \begin{itemize}
  \item $y^t_{\pi_t}$ is the output of the network at time $t$ (according to the path)
  \item The likelihood of a labeling $p(\boldsymbol{l}|\boldsymbol{X})$ can be computed efficiently using Fwd-Bwd over all permitted paths, using prefixes/ postfixes
  \item Also introduce an augmented label sequence $\boldsymbol{l'}$ which includes optional blanks
  \end{itemize}
\end{frame}

\begin{frame}{CTC Loss Function}
  \begin{align}
    O^{ML}(S,\mathcal{N}_W)&=-\sum_{(x,z) \in S} ln(p(z|x))
  \end{align}
  \begin{itemize}
  \item Loss function is neg-log prob of correctly labeling all observation/ labeling pairs $(x,z) \in S$
  \item $y = \mathcal{N}_w (x)$ is the mapping that the network induces 
  \item Consider each sequence pair independently, indexed by $t$
  \item Interestingly, the error can be backpropagated through this easily
  \end{itemize}
\end{frame}
% TODO: check this

\begin{frame}{What does this look like?}
  \begin{center}
    \includegraphics[height=35mm]{figures/ctc}
  \end{center}
  \tiny From \cite{graves2006connectionist}
\end{frame}

\begin{frame}{???}
  \begin{itemize}
  \item Originally, it was not clear how these things could be decoded efficiently
  \item Turns out, a wFST (weighted Finite State Transducer) framework works just fine \cite{eesen,sak2015fast}
  \item Need to carefully design the $T$ transducers for the tokens, which however replace the $H$ (HMM) and $C$ (context) transducers in conventional HMM-based systems
  \item Get class posteriors directly from label counts
  \item Advantage(s) at comparable accuracy
    \begin{itemize}
    \item CTC system is much smaller and usually faster during decoding
    \item Can use 30ms frame rate and works well with graphemic units
    \item N.B.: some of these findings might not be due to CTC, but to LSTMs, and may be back-ported
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Research Topics}
  \begin{itemize}
  \item What does this mean for spoken term detection?
    \begin{itemize}
    \item The lack of reliable timings is a problem
    \end{itemize}
  \item Do end-to-end approaches work in low resource scenarios?
    \begin{itemize}
    \item 10s of hours of training data, rather than 1000s?
    \end{itemize}
  \item Can we make ASR simpler?
  \item Can we build multi-lingual systems, use multi-task training?
  \item Can we use this to improve phone recognition
  \end{itemize}
\end{frame}

\begin{frame}{New Ideas}
  \begin{itemize}
  \item Combine CTC and attention-based models
    \begin{itemize}
    \item Use CTC as a front-end to attention-based models
    \item A la language independent feature extractors?
    \item Makes it easier for an encoder/ decoder model to learn
    \item Across languages?
    \end{itemize}
  \item Integrate non-auditory information
    \begin{itemize}
    \item Multimedia data is very rich, e.g.\,video
    \item Extract objects, scenes, speaker info from the visual part
    \item Use it to bootstrap, adapt, condition, or otherwise improve acoustic and/ or language models
    \end{itemize}
  \item Go beyond the speech-to-text paradigm
    \begin{itemize}
    \item Maybe a verbatim transcription is not always what we want
    \item Combine different information sources intelligently to give us audio-to-meaning in a new domain or language
    \end{itemize}
  \end{itemize}
\end{frame}


\section{Hands-On with Virtual Machines}

\begin{frame}
  \begin{center}
    {\color{Maroon}\Huge Hands-On Experience with Virtual Machines\par}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Practicalities}
  \begin{itemize}
  \item We want to give you hands-on experience with building ASR systems
  \item You will be able to train a system on a Babel language (most likely 201 Haitian)
  \item You can then experiment with other Babel languages, or port the system to other domains
  \item To facilitate experimentation, we will distribute a Virtual Machine (VM)
  \item {\color{Maroon}Read on to see how you can prepare}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Virtual Machines and Tools}
  \begin{itemize}
  \item Think of a VM as a ``virtual'' computer, in our case running Linux
  \item VMs allow sharing reproducible experiments easily
  \item \url{https://github.com/srvk}, \url{http://speechkitchen.org} as repositories
  \item \url{https://www.vagrantup.com/} to build VMs
  \item \url{https://www.virtualbox.org/} to run VMs (along with \url{https://aws.amazon.com/})
  \item An ``image'' is a computer when it is turned off, it becomes an ``instance'' when you turn it on
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Exercises}
  \begin{itemize}
  \item We will share a Vagrantfile, plus an image on AWS (most likely), and/ or a Virtualbox OVA (less likely)
  \item Your best bet is to run the exercise on AWS
  \item So, you may want to sign up for an account first (\url{https://aws.amazon.com/getting-started/})
  \item Familiarize yourself with how to start a Linux VM on ``EC2'' using a pre-configured Amazon Machine Image (AMI)
  \item Training a DNN-based recognizer on a GPU will cost some money, but the cost should not be dramatic
  \item Once you reproduced the basics, you can continue on AWS, or you can migrate to your own infrastructure
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Eesen}
  \begin{itemize}
  \item We will use the ``Eesen'' toolkit (\url{https://github.com/srvk/eesen}) for end-to-end speech recognition
  \item It is based on Kaldi (\url{http://kaldi-asr.org/}), but a bit smaller and easier to handle
  \item {\color{Maroon} More details to follow}
  \end{itemize}
\end{frame}

%\begin{frame}
%  \frametitle{References}    
%  \cite{quesst:icassp2015,metze:is2015,yajie-lstm:is2015,yajie-robust:is2015,yashesh:is2015,yajie:taslp2015,eesen,trecvid:2015,eesen-icassp,wang2016icassp,w4a:2016,icmr2016,miao:is2016,vms:is2016,shared:is2016,yash:is2016,e2echapter,dnnbook}
%\cite{dnnbook}
%\end{frame}
